{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Natural Language Processing GIF](https://raw.githubusercontent.com/FelixMatrixar/Basic-NLP-in-PyTorch/main/Natural%20Language%20Processing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*“Turning text into insight, one token at a time.”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tugas Pengantar Text Mining (Rekognisi)**\n",
    "---\n",
    "\n",
    "- **Nama :** Felix\n",
    "- **NIM :** M0721028\n",
    "- **Pengampu :** Mr. Fajar Muslim S.T., M.T.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Import Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini hanya melakukan import library math untuk pembuatan syntax TF-IDF dan BoW secara manual mengikuti rumus matematika dari metode masing-masing dan library sklearn untuk membantu melakukan modeling menggunakan **Decision Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Membuat Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap ini adalah membuat data contoh dan dipisahkan untuk memastikan mana variabel independen dan variabel dependen. Keterangan Label adalah sebagai berikut :\n",
    "- **P** artimya kalimat dengan sentimen **Positif**\n",
    "- **Ne** artimya kalimat dengan sentimen **Netral**\n",
    "- **N** artimya kalimat dengan sentimen **Negatif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"aku suka statistika\",\n",
    "    \"kelasnya bersih\",\n",
    "    \"AC-nya kurang dingin\",\n",
    "    \"papan tulisnya putih bersih\",\n",
    "    \"kursinya kurang empuk\"\n",
    "]\n",
    "\n",
    "labels = [\"P\", \"P\", \"N\", \"Ne\", \"N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Proses Tokenisasi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses tokenisasi mencakup lima tahap:\n",
    "\n",
    "1. Lowercase Conversion\n",
    "Karakter diubah menjadi huruf kecil berdasarkan kode ASCII.\n",
    "$$c' = c + 32, \\quad \\text{jika } 65 \\leq c \\leq 90$$\n",
    "\n",
    "2. Split on Space\n",
    "Teks dipisah menjadi kata berdasarkan spasi.\n",
    "$$\\text{Words} \\gets \\text{Words} \\cup \\{w\\}, \\quad \\text{jika } c = \\text{\" \"}$$\n",
    "\n",
    "3. Append to List\n",
    "Menambahkan elemen $𝑒$ ke dalam daftar $𝐿$\n",
    "$$L' = L + [e]$$\n",
    "\n",
    "4. Add to Set\n",
    "Menambahkan elemen $𝑒$ ke dalam himpunan $𝑆$ jika elemen belum ada.\n",
    "$$S' = S \\cup \\{e\\}, \\quad \\text{jika } e \\notin S$$\n",
    "\n",
    "5. Bubble Sort\n",
    "Mengurutkan daftar dengan menukar dua elemen jika tidak dalam urutan yang benar.\n",
    "$$\\text{Jika } L[j] > L[j+1], \\text{ maka } L[j], L[j+1] \\gets L[j+1], L[j]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vocab List ===\n",
      "['ac-nya', 'aku', 'bersih', 'dingin', 'empuk', 'kelasnya', 'kurang', 'kursinya', 'papan', 'putih', 'statistika', 'suka', 'tulisnya']\n"
     ]
    }
   ],
   "source": [
    "# Custom function to convert string to lowercase\n",
    "def custom_lower(text):\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        # If character is uppercase, convert to lowercase\n",
    "        if 'A' <= char <= 'Z':\n",
    "            result += chr(ord(char) + 32)\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "# Custom function to split a string into words\n",
    "def custom_split(text):\n",
    "    words = []\n",
    "    word = \"\"\n",
    "    for char in text:\n",
    "        if char == \" \":  # Split on space\n",
    "            if word:     # Add word if non-empty\n",
    "                words.append(word)\n",
    "                word = \"\"\n",
    "        else:\n",
    "            word += char\n",
    "    if word:  # Add the last word\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "# Custom function to append an element to a list\n",
    "def custom_append(lst, element):\n",
    "    lst += [element]  # Concatenate a single-element list\n",
    "    return lst\n",
    "\n",
    "# Custom function to add an element to a set\n",
    "def custom_add(s, element):\n",
    "    if element not in s:\n",
    "        s |= {element}  # Add element by creating a new set\n",
    "    return s\n",
    "\n",
    "# Custom function to sort a list\n",
    "def custom_sort(lst):\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(len(lst) - i - 1):\n",
    "            if lst[j] > lst[j + 1]:\n",
    "                lst[j], lst[j + 1] = lst[j + 1], lst[j]  # Swap\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Initialize vocab set and tokenized texts\n",
    "vocab_set = set()\n",
    "tokenized_texts = []\n",
    "\n",
    "# Tokenize each text\n",
    "for t in texts:\n",
    "    tokens = custom_split(custom_lower(t))  # Use custom lower and split\n",
    "    tokenized_texts = custom_append(tokenized_texts, tokens)  # Use custom append\n",
    "    for token in tokens:\n",
    "        vocab_set = custom_add(vocab_set, token)  # Use custom add\n",
    "\n",
    "# Convert vocab set to list and sort\n",
    "vocab_list = list(vocab_set)\n",
    "vocab_list = custom_sort(vocab_list)  # Use custom sort\n",
    "\n",
    "# Display vocab list\n",
    "print(\"=== Vocab List ===\")\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Proses TF-IDF dan BoW (Bag of Words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses TF-IDF mencakup:\n",
    "\n",
    "1. Term Frequency (TF): Mengukur seberapa sering kata $𝑡$ muncul dalam dokumen $𝑑$, dibandingkan dengan total kata dalam dokumen tersebut:\n",
    "$$\\text{TF}(t, d) = \\frac{\\text{count}(t, d)}{\\text{total\\_words}(d)}$$\n",
    "\n",
    "2. Inverse Document Frequency (IDF): Mengukur kelangkaan kata  $𝑡$ dalam koleksi dokumen $𝐷$. Jika `df($𝑡$)` adalah jumlah dokumen yang mengandung kata $𝑡$:\n",
    "$$\\text{IDF}(t, D) = \\log \\left( \\frac{N + 1}{\\text{df}(t) + 1} \\right) + 1$$\n",
    "\n",
    "3. TF-IDF: Kombinasi dari TF dan IDF:\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\cdot \\text{IDF}(t, D)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF Matrix ===\n",
      "Dokumen 0 (aku suka statistika): [0.0, 0.6995374295560366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6995374295560366, 0.6995374295560366, 0.0]\n",
      "Dokumen 1 (kelasnya bersih): [0.0, 0.0, 0.8465735902799727, 0.0, 0.0, 1.049306144334055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dokumen 2 (AC-nya kurang dingin): [0.6995374295560366, 0.0, 0.0, 0.6995374295560366, 0.0, 0.0, 0.5643823935199818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dokumen 3 (papan tulisnya putih bersih): [0.0, 0.0, 0.42328679513998635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5246530721670275, 0.5246530721670275, 0.0, 0.0, 0.5246530721670275]\n",
      "Dokumen 4 (kursinya kurang empuk): [0.0, 0.0, 0.0, 0.0, 0.6995374295560366, 0.0, 0.5643823935199818, 0.6995374295560366, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def TF_IDF(tokenized_docs, vocab):\n",
    "    \"\"\"\n",
    "    Fungsi untuk menghitung matriks TF-IDF dari dokumen yang sudah di-tokenisasi.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenized_docs (list of list of str): Daftar dokumen yang sudah di-tokenisasi.\n",
    "    - vocab (list of str): Kosakata unik dari semua dokumen.\n",
    "\n",
    "    Returns:\n",
    "    - list of list of float: Matriks TF-IDF, di mana baris adalah dokumen dan kolom adalah kosakata.\n",
    "    \"\"\"\n",
    "    N = len(tokenized_docs)  # Jumlah dokumen\n",
    "\n",
    "    # Langkah 1: Hitung DF untuk setiap kata dalam kosakata\n",
    "    df = {}\n",
    "    for v in vocab:\n",
    "        df[v] = 0\n",
    "    for tokens in tokenized_docs:\n",
    "        unique_tokens = set(tokens)  # Token unik dalam dokumen\n",
    "        for t in unique_tokens:\n",
    "            if t in df:\n",
    "                df[t] += 1\n",
    "\n",
    "    # Langkah 2: Hitung Matriks TF-IDF\n",
    "    tf_idf_matrix = []\n",
    "    for tokens in tokenized_docs:\n",
    "        row = []\n",
    "        total_kata = len(tokens)  # Total kata dalam dokumen\n",
    "        for v in vocab:\n",
    "            # Hitung TF\n",
    "            tf = tokens.count(v) / float(total_kata) if total_kata > 0 else 0\n",
    "            # Hitung IDF\n",
    "            idf = math.log((N + 1) / (df[v] + 1)) + 1\n",
    "            # Hitung TF-IDF\n",
    "            tf_idf = tf * idf\n",
    "            row.append(tf_idf)\n",
    "        tf_idf_matrix.append(row)\n",
    "    return tf_idf_matrix\n",
    "\n",
    "tfidf_matrix = TF_IDF(tokenized_texts, vocab_list)\n",
    "\n",
    "# Menampilkan hasil TF-IDF\n",
    "print(\"\\n=== TF-IDF Matrix ===\")\n",
    "for i, row in enumerate(tfidf_matrix):\n",
    "    print(f\"Dokumen {i} ({texts[i]}): {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses **Bag of Words (BOW)** mencakup tiga tahap:\n",
    "\n",
    "1. Membuat kosakata dengan cara mengumpulkan semua kata unik yang muncul di seluruh dokumen, kemudian menyusunnya ke dalam sebuah daftar yang diurutkan secara alfabetis.\n",
    "2. Menghitung frekuensi kemunculan setiap kata dalam kosakata untuk masing-masing dokumen. Jika sebuah kata tidak ditemukan dalam dokumen, frekuensinya akan bernilai 0.\n",
    "3. Menyusun Matriks BoW (Bag of Words) berdasarkan hasil perhitungan frekuensi kata. Pada matriks ini, setiap baris mewakili dokumen, setiap kolom mewakili kata dalam kosakata, dan nilai pada matriks menunjukkan frekuensi kemunculan kata tersebut di dokumen tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BoW Matrix ===\n",
      "Dokumen 0 (aku suka statistika): [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "Dokumen 1 (kelasnya bersih): [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Dokumen 2 (AC-nya kurang dingin): [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Dokumen 3 (papan tulisnya putih bersih): [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Dokumen 4 (kursinya kurang empuk): [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def BoW(tokenized_docs, vocab):\n",
    "    bow_matrix = []\n",
    "    for tokens in tokenized_docs:\n",
    "        row = []\n",
    "        for v in vocab:\n",
    "            count = tokens.count(v)\n",
    "            row.append(count)\n",
    "        bow_matrix.append(row)\n",
    "    return bow_matrix\n",
    "\n",
    "bow_matrix = BoW(tokenized_texts, vocab_list)\n",
    "\n",
    "# Menampilkan hasil BoW\n",
    "\n",
    "print(\"\\n=== BoW Matrix ===\")\n",
    "for i, row in enumerate(bow_matrix):\n",
    "    print(f\"Dokumen {i} ({texts[i]}): {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengubah label dari bentuk *string* ke *integer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"P\":0, \"N\":1, \"Ne\":2}\n",
    "y = [label_map[l] for l in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan split *train-test* untuk persiapan melakukan *modeling* dengan rasio 60:40. dengan 3 kalimat sebagai *data train* dan 2 kalimat sebagai *data test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_bow_tensor = torch.tensor(bow_matrix, dtype=torch.float32)\n",
    "X_tfidf_tensor = torch.tensor(tfidf_matrix, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dataset class to wrap features and labels\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets for BoW and TF-IDF\n",
    "dataset_bow = TextDataset(X_bow_tensor, y_tensor)\n",
    "dataset_tfidf = TextDataset(X_tfidf_tensor, y_tensor)\n",
    "\n",
    "# Split datasets into train and test sets (60:40 split)\n",
    "train_size = int(0.6 * len(dataset_bow))\n",
    "test_size = len(dataset_bow) - train_size\n",
    "train_dataset_bow, test_dataset_bow = random_split(dataset_bow, [train_size, test_size])\n",
    "train_dataset_tfidf, test_dataset_tfidf = random_split(dataset_tfidf, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "batch_size = 4\n",
    "train_loader_bow = DataLoader(train_dataset_bow, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bow = DataLoader(test_dataset_bow, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_tfidf = DataLoader(train_dataset_tfidf, batch_size=batch_size, shuffle=True)\n",
    "test_loader_tfidf = DataLoader(test_dataset_tfidf, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),  # First layer with 16 hidden units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, num_epochs=10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BoW Model:\n",
      "Epoch [1/200], Loss: 1.1661\n",
      "Epoch [2/200], Loss: 1.1604\n",
      "Epoch [3/200], Loss: 1.1548\n",
      "Epoch [4/200], Loss: 1.1492\n",
      "Epoch [5/200], Loss: 1.1437\n",
      "Epoch [6/200], Loss: 1.1381\n",
      "Epoch [7/200], Loss: 1.1326\n",
      "Epoch [8/200], Loss: 1.1271\n",
      "Epoch [9/200], Loss: 1.1216\n",
      "Epoch [10/200], Loss: 1.1162\n",
      "Epoch [11/200], Loss: 1.1109\n",
      "Epoch [12/200], Loss: 1.1057\n",
      "Epoch [13/200], Loss: 1.1004\n",
      "Epoch [14/200], Loss: 1.0952\n",
      "Epoch [15/200], Loss: 1.0900\n",
      "Epoch [16/200], Loss: 1.0847\n",
      "Epoch [17/200], Loss: 1.0795\n",
      "Epoch [18/200], Loss: 1.0743\n",
      "Epoch [19/200], Loss: 1.0690\n",
      "Epoch [20/200], Loss: 1.0638\n",
      "Epoch [21/200], Loss: 1.0586\n",
      "Epoch [22/200], Loss: 1.0534\n",
      "Epoch [23/200], Loss: 1.0482\n",
      "Epoch [24/200], Loss: 1.0429\n",
      "Epoch [25/200], Loss: 1.0377\n",
      "Epoch [26/200], Loss: 1.0325\n",
      "Epoch [27/200], Loss: 1.0272\n",
      "Epoch [28/200], Loss: 1.0220\n",
      "Epoch [29/200], Loss: 1.0167\n",
      "Epoch [30/200], Loss: 1.0114\n",
      "Epoch [31/200], Loss: 1.0061\n",
      "Epoch [32/200], Loss: 1.0008\n",
      "Epoch [33/200], Loss: 0.9954\n",
      "Epoch [34/200], Loss: 0.9901\n",
      "Epoch [35/200], Loss: 0.9847\n",
      "Epoch [36/200], Loss: 0.9793\n",
      "Epoch [37/200], Loss: 0.9739\n",
      "Epoch [38/200], Loss: 0.9685\n",
      "Epoch [39/200], Loss: 0.9632\n",
      "Epoch [40/200], Loss: 0.9580\n",
      "Epoch [41/200], Loss: 0.9528\n",
      "Epoch [42/200], Loss: 0.9476\n",
      "Epoch [43/200], Loss: 0.9423\n",
      "Epoch [44/200], Loss: 0.9370\n",
      "Epoch [45/200], Loss: 0.9316\n",
      "Epoch [46/200], Loss: 0.9263\n",
      "Epoch [47/200], Loss: 0.9209\n",
      "Epoch [48/200], Loss: 0.9155\n",
      "Epoch [49/200], Loss: 0.9100\n",
      "Epoch [50/200], Loss: 0.9045\n",
      "Epoch [51/200], Loss: 0.8990\n",
      "Epoch [52/200], Loss: 0.8935\n",
      "Epoch [53/200], Loss: 0.8880\n",
      "Epoch [54/200], Loss: 0.8825\n",
      "Epoch [55/200], Loss: 0.8769\n",
      "Epoch [56/200], Loss: 0.8712\n",
      "Epoch [57/200], Loss: 0.8656\n",
      "Epoch [58/200], Loss: 0.8600\n",
      "Epoch [59/200], Loss: 0.8544\n",
      "Epoch [60/200], Loss: 0.8487\n",
      "Epoch [61/200], Loss: 0.8430\n",
      "Epoch [62/200], Loss: 0.8373\n",
      "Epoch [63/200], Loss: 0.8316\n",
      "Epoch [64/200], Loss: 0.8258\n",
      "Epoch [65/200], Loss: 0.8200\n",
      "Epoch [66/200], Loss: 0.8141\n",
      "Epoch [67/200], Loss: 0.8082\n",
      "Epoch [68/200], Loss: 0.8023\n",
      "Epoch [69/200], Loss: 0.7965\n",
      "Epoch [70/200], Loss: 0.7905\n",
      "Epoch [71/200], Loss: 0.7845\n",
      "Epoch [72/200], Loss: 0.7785\n",
      "Epoch [73/200], Loss: 0.7725\n",
      "Epoch [74/200], Loss: 0.7665\n",
      "Epoch [75/200], Loss: 0.7605\n",
      "Epoch [76/200], Loss: 0.7547\n",
      "Epoch [77/200], Loss: 0.7488\n",
      "Epoch [78/200], Loss: 0.7429\n",
      "Epoch [79/200], Loss: 0.7370\n",
      "Epoch [80/200], Loss: 0.7311\n",
      "Epoch [81/200], Loss: 0.7252\n",
      "Epoch [82/200], Loss: 0.7192\n",
      "Epoch [83/200], Loss: 0.7133\n",
      "Epoch [84/200], Loss: 0.7074\n",
      "Epoch [85/200], Loss: 0.7015\n",
      "Epoch [86/200], Loss: 0.6956\n",
      "Epoch [87/200], Loss: 0.6897\n",
      "Epoch [88/200], Loss: 0.6838\n",
      "Epoch [89/200], Loss: 0.6778\n",
      "Epoch [90/200], Loss: 0.6719\n",
      "Epoch [91/200], Loss: 0.6660\n",
      "Epoch [92/200], Loss: 0.6601\n",
      "Epoch [93/200], Loss: 0.6540\n",
      "Epoch [94/200], Loss: 0.6479\n",
      "Epoch [95/200], Loss: 0.6417\n",
      "Epoch [96/200], Loss: 0.6355\n",
      "Epoch [97/200], Loss: 0.6292\n",
      "Epoch [98/200], Loss: 0.6230\n",
      "Epoch [99/200], Loss: 0.6168\n",
      "Epoch [100/200], Loss: 0.6106\n",
      "Epoch [101/200], Loss: 0.6044\n",
      "Epoch [102/200], Loss: 0.5982\n",
      "Epoch [103/200], Loss: 0.5921\n",
      "Epoch [104/200], Loss: 0.5860\n",
      "Epoch [105/200], Loss: 0.5799\n",
      "Epoch [106/200], Loss: 0.5738\n",
      "Epoch [107/200], Loss: 0.5678\n",
      "Epoch [108/200], Loss: 0.5618\n",
      "Epoch [109/200], Loss: 0.5559\n",
      "Epoch [110/200], Loss: 0.5500\n",
      "Epoch [111/200], Loss: 0.5441\n",
      "Epoch [112/200], Loss: 0.5383\n",
      "Epoch [113/200], Loss: 0.5325\n",
      "Epoch [114/200], Loss: 0.5267\n",
      "Epoch [115/200], Loss: 0.5210\n",
      "Epoch [116/200], Loss: 0.5154\n",
      "Epoch [117/200], Loss: 0.5097\n",
      "Epoch [118/200], Loss: 0.5040\n",
      "Epoch [119/200], Loss: 0.4982\n",
      "Epoch [120/200], Loss: 0.4925\n",
      "Epoch [121/200], Loss: 0.4868\n",
      "Epoch [122/200], Loss: 0.4812\n",
      "Epoch [123/200], Loss: 0.4756\n",
      "Epoch [124/200], Loss: 0.4700\n",
      "Epoch [125/200], Loss: 0.4645\n",
      "Epoch [126/200], Loss: 0.4590\n",
      "Epoch [127/200], Loss: 0.4536\n",
      "Epoch [128/200], Loss: 0.4483\n",
      "Epoch [129/200], Loss: 0.4430\n",
      "Epoch [130/200], Loss: 0.4377\n",
      "Epoch [131/200], Loss: 0.4325\n",
      "Epoch [132/200], Loss: 0.4274\n",
      "Epoch [133/200], Loss: 0.4223\n",
      "Epoch [134/200], Loss: 0.4173\n",
      "Epoch [135/200], Loss: 0.4123\n",
      "Epoch [136/200], Loss: 0.4074\n",
      "Epoch [137/200], Loss: 0.4025\n",
      "Epoch [138/200], Loss: 0.3978\n",
      "Epoch [139/200], Loss: 0.3932\n",
      "Epoch [140/200], Loss: 0.3886\n",
      "Epoch [141/200], Loss: 0.3841\n",
      "Epoch [142/200], Loss: 0.3796\n",
      "Epoch [143/200], Loss: 0.3752\n",
      "Epoch [144/200], Loss: 0.3708\n",
      "Epoch [145/200], Loss: 0.3664\n",
      "Epoch [146/200], Loss: 0.3621\n",
      "Epoch [147/200], Loss: 0.3579\n",
      "Epoch [148/200], Loss: 0.3537\n",
      "Epoch [149/200], Loss: 0.3496\n",
      "Epoch [150/200], Loss: 0.3455\n",
      "Epoch [151/200], Loss: 0.3415\n",
      "Epoch [152/200], Loss: 0.3376\n",
      "Epoch [153/200], Loss: 0.3337\n",
      "Epoch [154/200], Loss: 0.3298\n",
      "Epoch [155/200], Loss: 0.3260\n",
      "Epoch [156/200], Loss: 0.3223\n",
      "Epoch [157/200], Loss: 0.3185\n",
      "Epoch [158/200], Loss: 0.3148\n",
      "Epoch [159/200], Loss: 0.3112\n",
      "Epoch [160/200], Loss: 0.3076\n",
      "Epoch [161/200], Loss: 0.3041\n",
      "Epoch [162/200], Loss: 0.3006\n",
      "Epoch [163/200], Loss: 0.2972\n",
      "Epoch [164/200], Loss: 0.2938\n",
      "Epoch [165/200], Loss: 0.2904\n",
      "Epoch [166/200], Loss: 0.2871\n",
      "Epoch [167/200], Loss: 0.2839\n",
      "Epoch [168/200], Loss: 0.2806\n",
      "Epoch [169/200], Loss: 0.2774\n",
      "Epoch [170/200], Loss: 0.2742\n",
      "Epoch [171/200], Loss: 0.2711\n",
      "Epoch [172/200], Loss: 0.2680\n",
      "Epoch [173/200], Loss: 0.2650\n",
      "Epoch [174/200], Loss: 0.2620\n",
      "Epoch [175/200], Loss: 0.2590\n",
      "Epoch [176/200], Loss: 0.2561\n",
      "Epoch [177/200], Loss: 0.2532\n",
      "Epoch [178/200], Loss: 0.2503\n",
      "Epoch [179/200], Loss: 0.2475\n",
      "Epoch [180/200], Loss: 0.2447\n",
      "Epoch [181/200], Loss: 0.2419\n",
      "Epoch [182/200], Loss: 0.2392\n",
      "Epoch [183/200], Loss: 0.2365\n",
      "Epoch [184/200], Loss: 0.2338\n",
      "Epoch [185/200], Loss: 0.2312\n",
      "Epoch [186/200], Loss: 0.2286\n",
      "Epoch [187/200], Loss: 0.2260\n",
      "Epoch [188/200], Loss: 0.2235\n",
      "Epoch [189/200], Loss: 0.2211\n",
      "Epoch [190/200], Loss: 0.2187\n",
      "Epoch [191/200], Loss: 0.2164\n",
      "Epoch [192/200], Loss: 0.2140\n",
      "Epoch [193/200], Loss: 0.2118\n",
      "Epoch [194/200], Loss: 0.2095\n",
      "Epoch [195/200], Loss: 0.2072\n",
      "Epoch [196/200], Loss: 0.2050\n",
      "Epoch [197/200], Loss: 0.2028\n",
      "Epoch [198/200], Loss: 0.2006\n",
      "Epoch [199/200], Loss: 0.1985\n",
      "Epoch [200/200], Loss: 0.1963\n",
      "Accuracy (BoW): 0.5000\n",
      "\n",
      "Training TF-IDF Model:\n",
      "Epoch [1/200], Loss: 1.1132\n",
      "Epoch [2/200], Loss: 1.1112\n",
      "Epoch [3/200], Loss: 1.1092\n",
      "Epoch [4/200], Loss: 1.1072\n",
      "Epoch [5/200], Loss: 1.1052\n",
      "Epoch [6/200], Loss: 1.1032\n",
      "Epoch [7/200], Loss: 1.1012\n",
      "Epoch [8/200], Loss: 1.0992\n",
      "Epoch [9/200], Loss: 1.0973\n",
      "Epoch [10/200], Loss: 1.0953\n",
      "Epoch [11/200], Loss: 1.0935\n",
      "Epoch [12/200], Loss: 1.0918\n",
      "Epoch [13/200], Loss: 1.0902\n",
      "Epoch [14/200], Loss: 1.0885\n",
      "Epoch [15/200], Loss: 1.0869\n",
      "Epoch [16/200], Loss: 1.0852\n",
      "Epoch [17/200], Loss: 1.0836\n",
      "Epoch [18/200], Loss: 1.0819\n",
      "Epoch [19/200], Loss: 1.0803\n",
      "Epoch [20/200], Loss: 1.0786\n",
      "Epoch [21/200], Loss: 1.0770\n",
      "Epoch [22/200], Loss: 1.0753\n",
      "Epoch [23/200], Loss: 1.0737\n",
      "Epoch [24/200], Loss: 1.0722\n",
      "Epoch [25/200], Loss: 1.0706\n",
      "Epoch [26/200], Loss: 1.0690\n",
      "Epoch [27/200], Loss: 1.0673\n",
      "Epoch [28/200], Loss: 1.0657\n",
      "Epoch [29/200], Loss: 1.0641\n",
      "Epoch [30/200], Loss: 1.0624\n",
      "Epoch [31/200], Loss: 1.0607\n",
      "Epoch [32/200], Loss: 1.0590\n",
      "Epoch [33/200], Loss: 1.0574\n",
      "Epoch [34/200], Loss: 1.0557\n",
      "Epoch [35/200], Loss: 1.0540\n",
      "Epoch [36/200], Loss: 1.0523\n",
      "Epoch [37/200], Loss: 1.0506\n",
      "Epoch [38/200], Loss: 1.0489\n",
      "Epoch [39/200], Loss: 1.0471\n",
      "Epoch [40/200], Loss: 1.0453\n",
      "Epoch [41/200], Loss: 1.0435\n",
      "Epoch [42/200], Loss: 1.0416\n",
      "Epoch [43/200], Loss: 1.0398\n",
      "Epoch [44/200], Loss: 1.0379\n",
      "Epoch [45/200], Loss: 1.0360\n",
      "Epoch [46/200], Loss: 1.0341\n",
      "Epoch [47/200], Loss: 1.0321\n",
      "Epoch [48/200], Loss: 1.0302\n",
      "Epoch [49/200], Loss: 1.0283\n",
      "Epoch [50/200], Loss: 1.0263\n",
      "Epoch [51/200], Loss: 1.0243\n",
      "Epoch [52/200], Loss: 1.0222\n",
      "Epoch [53/200], Loss: 1.0202\n",
      "Epoch [54/200], Loss: 1.0181\n",
      "Epoch [55/200], Loss: 1.0160\n",
      "Epoch [56/200], Loss: 1.0138\n",
      "Epoch [57/200], Loss: 1.0117\n",
      "Epoch [58/200], Loss: 1.0094\n",
      "Epoch [59/200], Loss: 1.0071\n",
      "Epoch [60/200], Loss: 1.0048\n",
      "Epoch [61/200], Loss: 1.0024\n",
      "Epoch [62/200], Loss: 1.0001\n",
      "Epoch [63/200], Loss: 0.9977\n",
      "Epoch [64/200], Loss: 0.9952\n",
      "Epoch [65/200], Loss: 0.9926\n",
      "Epoch [66/200], Loss: 0.9900\n",
      "Epoch [67/200], Loss: 0.9873\n",
      "Epoch [68/200], Loss: 0.9845\n",
      "Epoch [69/200], Loss: 0.9818\n",
      "Epoch [70/200], Loss: 0.9791\n",
      "Epoch [71/200], Loss: 0.9763\n",
      "Epoch [72/200], Loss: 0.9734\n",
      "Epoch [73/200], Loss: 0.9705\n",
      "Epoch [74/200], Loss: 0.9675\n",
      "Epoch [75/200], Loss: 0.9646\n",
      "Epoch [76/200], Loss: 0.9616\n",
      "Epoch [77/200], Loss: 0.9585\n",
      "Epoch [78/200], Loss: 0.9554\n",
      "Epoch [79/200], Loss: 0.9524\n",
      "Epoch [80/200], Loss: 0.9492\n",
      "Epoch [81/200], Loss: 0.9460\n",
      "Epoch [82/200], Loss: 0.9428\n",
      "Epoch [83/200], Loss: 0.9396\n",
      "Epoch [84/200], Loss: 0.9364\n",
      "Epoch [85/200], Loss: 0.9331\n",
      "Epoch [86/200], Loss: 0.9298\n",
      "Epoch [87/200], Loss: 0.9264\n",
      "Epoch [88/200], Loss: 0.9230\n",
      "Epoch [89/200], Loss: 0.9196\n",
      "Epoch [90/200], Loss: 0.9162\n",
      "Epoch [91/200], Loss: 0.9129\n",
      "Epoch [92/200], Loss: 0.9095\n",
      "Epoch [93/200], Loss: 0.9062\n",
      "Epoch [94/200], Loss: 0.9027\n",
      "Epoch [95/200], Loss: 0.8992\n",
      "Epoch [96/200], Loss: 0.8957\n",
      "Epoch [97/200], Loss: 0.8922\n",
      "Epoch [98/200], Loss: 0.8886\n",
      "Epoch [99/200], Loss: 0.8850\n",
      "Epoch [100/200], Loss: 0.8814\n",
      "Epoch [101/200], Loss: 0.8777\n",
      "Epoch [102/200], Loss: 0.8741\n",
      "Epoch [103/200], Loss: 0.8704\n",
      "Epoch [104/200], Loss: 0.8667\n",
      "Epoch [105/200], Loss: 0.8630\n",
      "Epoch [106/200], Loss: 0.8593\n",
      "Epoch [107/200], Loss: 0.8556\n",
      "Epoch [108/200], Loss: 0.8518\n",
      "Epoch [109/200], Loss: 0.8480\n",
      "Epoch [110/200], Loss: 0.8442\n",
      "Epoch [111/200], Loss: 0.8403\n",
      "Epoch [112/200], Loss: 0.8365\n",
      "Epoch [113/200], Loss: 0.8326\n",
      "Epoch [114/200], Loss: 0.8287\n",
      "Epoch [115/200], Loss: 0.8248\n",
      "Epoch [116/200], Loss: 0.8210\n",
      "Epoch [117/200], Loss: 0.8170\n",
      "Epoch [118/200], Loss: 0.8130\n",
      "Epoch [119/200], Loss: 0.8091\n",
      "Epoch [120/200], Loss: 0.8051\n",
      "Epoch [121/200], Loss: 0.8011\n",
      "Epoch [122/200], Loss: 0.7971\n",
      "Epoch [123/200], Loss: 0.7931\n",
      "Epoch [124/200], Loss: 0.7891\n",
      "Epoch [125/200], Loss: 0.7850\n",
      "Epoch [126/200], Loss: 0.7810\n",
      "Epoch [127/200], Loss: 0.7769\n",
      "Epoch [128/200], Loss: 0.7728\n",
      "Epoch [129/200], Loss: 0.7688\n",
      "Epoch [130/200], Loss: 0.7647\n",
      "Epoch [131/200], Loss: 0.7605\n",
      "Epoch [132/200], Loss: 0.7564\n",
      "Epoch [133/200], Loss: 0.7523\n",
      "Epoch [134/200], Loss: 0.7482\n",
      "Epoch [135/200], Loss: 0.7440\n",
      "Epoch [136/200], Loss: 0.7399\n",
      "Epoch [137/200], Loss: 0.7357\n",
      "Epoch [138/200], Loss: 0.7316\n",
      "Epoch [139/200], Loss: 0.7274\n",
      "Epoch [140/200], Loss: 0.7232\n",
      "Epoch [141/200], Loss: 0.7191\n",
      "Epoch [142/200], Loss: 0.7149\n",
      "Epoch [143/200], Loss: 0.7107\n",
      "Epoch [144/200], Loss: 0.7066\n",
      "Epoch [145/200], Loss: 0.7024\n",
      "Epoch [146/200], Loss: 0.6982\n",
      "Epoch [147/200], Loss: 0.6941\n",
      "Epoch [148/200], Loss: 0.6899\n",
      "Epoch [149/200], Loss: 0.6857\n",
      "Epoch [150/200], Loss: 0.6816\n",
      "Epoch [151/200], Loss: 0.6774\n",
      "Epoch [152/200], Loss: 0.6733\n",
      "Epoch [153/200], Loss: 0.6692\n",
      "Epoch [154/200], Loss: 0.6650\n",
      "Epoch [155/200], Loss: 0.6609\n",
      "Epoch [156/200], Loss: 0.6568\n",
      "Epoch [157/200], Loss: 0.6527\n",
      "Epoch [158/200], Loss: 0.6485\n",
      "Epoch [159/200], Loss: 0.6444\n",
      "Epoch [160/200], Loss: 0.6403\n",
      "Epoch [161/200], Loss: 0.6363\n",
      "Epoch [162/200], Loss: 0.6322\n",
      "Epoch [163/200], Loss: 0.6281\n",
      "Epoch [164/200], Loss: 0.6240\n",
      "Epoch [165/200], Loss: 0.6200\n",
      "Epoch [166/200], Loss: 0.6159\n",
      "Epoch [167/200], Loss: 0.6119\n",
      "Epoch [168/200], Loss: 0.6079\n",
      "Epoch [169/200], Loss: 0.6039\n",
      "Epoch [170/200], Loss: 0.5999\n",
      "Epoch [171/200], Loss: 0.5959\n",
      "Epoch [172/200], Loss: 0.5919\n",
      "Epoch [173/200], Loss: 0.5879\n",
      "Epoch [174/200], Loss: 0.5840\n",
      "Epoch [175/200], Loss: 0.5801\n",
      "Epoch [176/200], Loss: 0.5761\n",
      "Epoch [177/200], Loss: 0.5722\n",
      "Epoch [178/200], Loss: 0.5683\n",
      "Epoch [179/200], Loss: 0.5644\n",
      "Epoch [180/200], Loss: 0.5606\n",
      "Epoch [181/200], Loss: 0.5567\n",
      "Epoch [182/200], Loss: 0.5529\n",
      "Epoch [183/200], Loss: 0.5490\n",
      "Epoch [184/200], Loss: 0.5452\n",
      "Epoch [185/200], Loss: 0.5414\n",
      "Epoch [186/200], Loss: 0.5376\n",
      "Epoch [187/200], Loss: 0.5339\n",
      "Epoch [188/200], Loss: 0.5302\n",
      "Epoch [189/200], Loss: 0.5264\n",
      "Epoch [190/200], Loss: 0.5227\n",
      "Epoch [191/200], Loss: 0.5190\n",
      "Epoch [192/200], Loss: 0.5153\n",
      "Epoch [193/200], Loss: 0.5116\n",
      "Epoch [194/200], Loss: 0.5079\n",
      "Epoch [195/200], Loss: 0.5043\n",
      "Epoch [196/200], Loss: 0.5007\n",
      "Epoch [197/200], Loss: 0.4971\n",
      "Epoch [198/200], Loss: 0.4935\n",
      "Epoch [199/200], Loss: 0.4900\n",
      "Epoch [200/200], Loss: 0.4864\n",
      "Accuracy (TF-IDF): 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim_bow = X_bow_tensor.shape[1]  # Number of features in BoW\n",
    "input_dim_tfidf = X_tfidf_tensor.shape[1]  # Number of features in TF-IDF\n",
    "num_classes = len(label_map)  # Number of output classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "\n",
    "# Model, loss, and optimizer for BoW\n",
    "model_bow = SimpleNN(input_dim_bow, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_bow = optim.Adam(model_bow.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nTraining BoW Model:\")\n",
    "train_model(model_bow, optimizer_bow, criterion, train_loader_bow, num_epochs)\n",
    "\n",
    "# Evaluate BoW Model\n",
    "accuracy_bow = evaluate_model(model_bow, test_loader_bow)\n",
    "print(f\"Accuracy (BoW): {accuracy_bow:.4f}\")\n",
    "\n",
    "# Model, loss, and optimizer for TF-IDF\n",
    "model_tfidf = SimpleNN(input_dim_tfidf, num_classes)\n",
    "optimizer_tfidf = optim.Adam(model_tfidf.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nTraining TF-IDF Model:\")\n",
    "train_model(model_tfidf, optimizer_tfidf, criterion, train_loader_tfidf, num_epochs)\n",
    "\n",
    "# Evaluate TF-IDF Model\n",
    "accuracy_tfidf = evaluate_model(model_tfidf, test_loader_tfidf)\n",
    "print(f\"Accuracy (TF-IDF): {accuracy_tfidf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, architecture):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        layers = []\n",
    "        for i, (in_dim, out_dim) in enumerate(architecture):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if i < len(architecture) - 1:  # Add activation and dropout for hidden layers\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.3))\n",
    "        layers.append(nn.Linear(architecture[-1][1], num_classes))  # Output layer\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures: List of tuples (input_dim, output_dim)\n",
    "architectures = {\n",
    "    \"Small\": [(input_dim_bow, 16), (16, 8)],\n",
    "    \"Medium\": [(input_dim_bow, 32), (32, 16), (16, 8)],\n",
    "    \"Large\": [(input_dim_bow, 64), (64, 32), (32, 16)],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, criterion, train_loader, test_loader, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BoW Model - Architecture: Small\n",
      "Epoch [1/20], Loss: 1.1007\n",
      "Epoch [2/20], Loss: 1.0741\n",
      "Epoch [3/20], Loss: 1.0336\n",
      "Epoch [4/20], Loss: 1.0406\n",
      "Epoch [5/20], Loss: 0.9878\n",
      "Epoch [6/20], Loss: 0.9267\n",
      "Epoch [7/20], Loss: 0.9077\n",
      "Epoch [8/20], Loss: 0.8677\n",
      "Epoch [9/20], Loss: 0.8121\n",
      "Epoch [10/20], Loss: 0.7794\n",
      "Epoch [11/20], Loss: 0.7548\n",
      "Epoch [12/20], Loss: 0.7109\n",
      "Epoch [13/20], Loss: 0.6724\n",
      "Epoch [14/20], Loss: 0.5916\n",
      "Epoch [15/20], Loss: 0.5890\n",
      "Epoch [16/20], Loss: 0.6362\n",
      "Epoch [17/20], Loss: 0.6157\n",
      "Epoch [18/20], Loss: 0.4026\n",
      "Epoch [19/20], Loss: 0.4531\n",
      "Epoch [20/20], Loss: 0.4973\n",
      "\n",
      "Training TF-IDF Model - Architecture: Small\n",
      "Epoch [1/20], Loss: 1.1278\n",
      "Epoch [2/20], Loss: 1.1250\n",
      "Epoch [3/20], Loss: 1.1136\n",
      "Epoch [4/20], Loss: 1.1080\n",
      "Epoch [5/20], Loss: 1.0710\n",
      "Epoch [6/20], Loss: 1.0650\n",
      "Epoch [7/20], Loss: 1.0689\n",
      "Epoch [8/20], Loss: 1.0501\n",
      "Epoch [9/20], Loss: 1.0755\n",
      "Epoch [10/20], Loss: 1.0114\n",
      "Epoch [11/20], Loss: 0.9822\n",
      "Epoch [12/20], Loss: 0.9921\n",
      "Epoch [13/20], Loss: 0.9965\n",
      "Epoch [14/20], Loss: 0.9252\n",
      "Epoch [15/20], Loss: 0.8638\n",
      "Epoch [16/20], Loss: 0.8973\n",
      "Epoch [17/20], Loss: 0.9295\n",
      "Epoch [18/20], Loss: 0.9959\n",
      "Epoch [19/20], Loss: 0.8619\n",
      "Epoch [20/20], Loss: 0.8621\n",
      "\n",
      "Training BoW Model - Architecture: Medium\n",
      "Epoch [1/20], Loss: 1.1400\n",
      "Epoch [2/20], Loss: 1.1005\n",
      "Epoch [3/20], Loss: 1.0448\n",
      "Epoch [4/20], Loss: 0.9947\n",
      "Epoch [5/20], Loss: 0.9861\n",
      "Epoch [6/20], Loss: 0.9129\n",
      "Epoch [7/20], Loss: 0.9048\n",
      "Epoch [8/20], Loss: 0.8051\n",
      "Epoch [9/20], Loss: 0.8585\n",
      "Epoch [10/20], Loss: 0.8254\n",
      "Epoch [11/20], Loss: 0.6431\n",
      "Epoch [12/20], Loss: 0.6433\n",
      "Epoch [13/20], Loss: 0.6416\n",
      "Epoch [14/20], Loss: 0.5352\n",
      "Epoch [15/20], Loss: 0.7139\n",
      "Epoch [16/20], Loss: 0.5438\n",
      "Epoch [17/20], Loss: 0.4728\n",
      "Epoch [18/20], Loss: 0.4060\n",
      "Epoch [19/20], Loss: 0.3914\n",
      "Epoch [20/20], Loss: 0.3310\n",
      "\n",
      "Training TF-IDF Model - Architecture: Medium\n",
      "Epoch [1/20], Loss: 1.1116\n",
      "Epoch [2/20], Loss: 1.0964\n",
      "Epoch [3/20], Loss: 1.1147\n",
      "Epoch [4/20], Loss: 1.1201\n",
      "Epoch [5/20], Loss: 1.0740\n",
      "Epoch [6/20], Loss: 1.0932\n",
      "Epoch [7/20], Loss: 1.0516\n",
      "Epoch [8/20], Loss: 1.0488\n",
      "Epoch [9/20], Loss: 1.0562\n",
      "Epoch [10/20], Loss: 1.0510\n",
      "Epoch [11/20], Loss: 1.0198\n",
      "Epoch [12/20], Loss: 0.9225\n",
      "Epoch [13/20], Loss: 0.9320\n",
      "Epoch [14/20], Loss: 0.9410\n",
      "Epoch [15/20], Loss: 1.0083\n",
      "Epoch [16/20], Loss: 0.8723\n",
      "Epoch [17/20], Loss: 0.8438\n",
      "Epoch [18/20], Loss: 0.7359\n",
      "Epoch [19/20], Loss: 0.7758\n",
      "Epoch [20/20], Loss: 0.7108\n",
      "\n",
      "Training BoW Model - Architecture: Large\n",
      "Epoch [1/20], Loss: 1.1026\n",
      "Epoch [2/20], Loss: 1.0423\n",
      "Epoch [3/20], Loss: 0.9845\n",
      "Epoch [4/20], Loss: 0.9258\n",
      "Epoch [5/20], Loss: 0.9359\n",
      "Epoch [6/20], Loss: 0.8369\n",
      "Epoch [7/20], Loss: 0.7665\n",
      "Epoch [8/20], Loss: 0.7588\n",
      "Epoch [9/20], Loss: 0.6192\n",
      "Epoch [10/20], Loss: 0.4354\n",
      "Epoch [11/20], Loss: 0.3005\n",
      "Epoch [12/20], Loss: 0.1838\n",
      "Epoch [13/20], Loss: 0.2545\n",
      "Epoch [14/20], Loss: 0.1146\n",
      "Epoch [15/20], Loss: 0.0760\n",
      "Epoch [16/20], Loss: 0.0137\n",
      "Epoch [17/20], Loss: 0.1485\n",
      "Epoch [18/20], Loss: 0.0005\n",
      "Epoch [19/20], Loss: 0.0044\n",
      "Epoch [20/20], Loss: 0.0188\n",
      "\n",
      "Training TF-IDF Model - Architecture: Large\n",
      "Epoch [1/20], Loss: 1.0941\n",
      "Epoch [2/20], Loss: 1.0966\n",
      "Epoch [3/20], Loss: 1.0650\n",
      "Epoch [4/20], Loss: 1.0502\n",
      "Epoch [5/20], Loss: 1.0720\n",
      "Epoch [6/20], Loss: 1.0190\n",
      "Epoch [7/20], Loss: 0.9926\n",
      "Epoch [8/20], Loss: 0.9751\n",
      "Epoch [9/20], Loss: 0.8374\n",
      "Epoch [10/20], Loss: 0.9478\n",
      "Epoch [11/20], Loss: 0.7409\n",
      "Epoch [12/20], Loss: 0.6031\n",
      "Epoch [13/20], Loss: 0.6183\n",
      "Epoch [14/20], Loss: 0.4334\n",
      "Epoch [15/20], Loss: 0.4592\n",
      "Epoch [16/20], Loss: 0.4746\n",
      "Epoch [17/20], Loss: 0.2457\n",
      "Epoch [18/20], Loss: 0.0857\n",
      "Epoch [19/20], Loss: 0.1136\n",
      "Epoch [20/20], Loss: 0.1967\n",
      "  Architecture  BoW Accuracy  TF-IDF Accuracy\n",
      "0        Small           0.0              0.5\n",
      "1       Medium           0.0              0.5\n",
      "2        Large           0.5              0.5\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for arch_name, arch in architectures.items():\n",
    "    # Train and evaluate on BoW\n",
    "    model_bow = ImprovedNN(input_dim_bow, num_classes, arch)\n",
    "    optimizer_bow = optim.Adam(model_bow.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(f\"\\nTraining BoW Model - Architecture: {arch_name}\")\n",
    "    acc_bow = train_and_evaluate(model_bow, optimizer_bow, criterion, train_loader_bow, test_loader_bow)\n",
    "\n",
    "    # Train and evaluate on TF-IDF\n",
    "    model_tfidf = ImprovedNN(input_dim_tfidf, num_classes, arch)\n",
    "    optimizer_tfidf = optim.Adam(model_tfidf.parameters(), lr=0.01)\n",
    "    print(f\"\\nTraining TF-IDF Model - Architecture: {arch_name}\")\n",
    "    acc_tfidf = train_and_evaluate(model_tfidf, optimizer_tfidf, criterion, train_loader_tfidf, test_loader_tfidf)\n",
    "\n",
    "    # Save results\n",
    "    results.append({\"Architecture\": arch_name, \"BoW Accuracy\": acc_bow, \"TF-IDF Accuracy\": acc_tfidf})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Kesimpulan**\n",
    "\n",
    "<div align='justify'>\n",
    "&emsp;&emsp;&emsp;&emsp;\n",
    "Dari hasil evaluasi model dengan berbagai arsitektur neural network (Small, Medium, dan Large), dapat dilihat bahwa representasi TF-IDF konsisten menghasilkan akurasi 0.5 untuk semua arsitektur, sementara representasi BoW hanya memberikan akurasi yang lebih tinggi (0.5) pada arsitektur Large. Hal ini menunjukkan bahwa model yang lebih kompleks, seperti arsitektur Large dengan lebih banyak lapisan dan unit tersembunyi, dapat memanfaatkan representasi BoW secara lebih efektif dibandingkan arsitektur yang lebih sederhana (Small dan Medium). Akan tetapi, untuk representasi TF-IDF, akurasi tetap stabil di semua arsitektur, mengindikasikan bahwa model tidak dapat menangkap informasi tambahan yang lebih baik meskipun kapasitas model meningkat. Kesimpulannya, arsitektur Large memberikan performa terbaik secara keseluruhan, terutama dalam mengolah representasi BoW, tetapi representasi TF-IDF lebih konsisten dan stabil terlepas dari kompleksitas model. Untuk peningkatan lebih lanjut, eksplorasi terhadap data, representasi, atau penyesuaian hiperparameter dapat dilakukan.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
