{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tugas Pengantar Text Mining (Rekognisi)**\n",
    "---\n",
    "\n",
    "- **Nama :** Felix\n",
    "- **NIM :** M0721028\n",
    "- **Pengampu :** Mr. Fajar Muslim S.T., M.T.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Import Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini hanya melakukan import library math untuk pembuatan syntax TF-IDF dan BoW secara manual mengikuti rumus matematika dari metode masing-masing dan library sklearn untuk membantu melakukan modeling menggunakan **Decision Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Membuat Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap ini adalah membuat data contoh dan dipisahkan untuk memastikan mana variabel independen dan variabel dependen. Keterangan Label adalah sebagai berikut :\n",
    "- **P** artimya kalimat dengan sentimen **Positif**\n",
    "- **Ne** artimya kalimat dengan sentimen **Netral**\n",
    "- **N** artimya kalimat dengan sentimen **Negatif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"aku suka statistika\",\n",
    "    \"kelasnya bersih\",\n",
    "    \"AC-nya kurang dingin\",\n",
    "    \"papan tulisnya putih bersih\",\n",
    "    \"kursinya kurang empuk\"\n",
    "]\n",
    "\n",
    "labels = [\"P\", \"P\", \"N\", \"Ne\", \"N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Proses Tokenisasi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses ini terdapat 3 tahap yang berbeda, yaitu :\n",
    "\n",
    "1. Membuat fungsi/definisi tokenisasi untuk memisahkan tiap kata dan juga mengaplikasikan Lower Case untuk semua kata.\n",
    "2. Mengaplikasikan fungsi yang sudah dibuat untuk menyimpan kata yang \"dipisahkan\" di `tokenized_texts` dan menyimpan kata unik di `vocab_set`.\n",
    "3. Menjadikan kata unik yang telah disimpan tadi dalam bentuk **list** dan dilakukan `sort` untuk mengurutkan list kata secara alfabetikal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vocab List ===\n",
      "['ac-nya', 'aku', 'bersih', 'dingin', 'empuk', 'kelasnya', 'kurang', 'kursinya', 'papan', 'putih', 'statistika', 'suka', 'tulisnya']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "vocab_set = set()\n",
    "tokenized_texts = []\n",
    "for t in texts:\n",
    "    tokens = tokenize(t)\n",
    "    tokenized_texts.append(tokens)\n",
    "    for token in tokens:\n",
    "        vocab_set.add(token)\n",
    "\n",
    "vocab_list = list(vocab_set)\n",
    "vocab_list.sort()\n",
    "\n",
    "# Menampilkan Vocab List\n",
    "print(\"=== Vocab List ===\")\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Proses TF-IDF dan BoW (Bag of Words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses perhitungan **TF-IDF** terdapat 4 tahap, yaitu:\n",
    "\n",
    "1. Menghitung jumlah kemunculan setiap kata dalam dokumen, lalu membaginya dengan total kata dalam dokumen tersebut.  \n",
    "2. Menghitung berapa banyak dokumen yang mengandung setiap kata unik dari semua dokumen.  \n",
    "3. Menghitung bobot berdasarkan seberapa jarang kata muncul di seluruh dokumen, dimana semakin jarang kata tersebut muncul, semakin tinggi bobotnya.\n",
    "4. Mengalikan hasil **TF** dan **IDF** untuk mendapatkan nilai akhir, nilai ini akan digunakan untuk mewakili seberapa penting sebuah kata dalam dokumen tertentu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF Matrix ===\n",
      "Dokumen 0 (aku suka statistika): [0.0, 0.6995374295560366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6995374295560366, 0.6995374295560366, 0.0]\n",
      "Dokumen 1 (kelasnya bersih): [0.0, 0.0, 0.8465735902799727, 0.0, 0.0, 1.049306144334055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dokumen 2 (AC-nya kurang dingin): [0.6995374295560366, 0.0, 0.0, 0.6995374295560366, 0.0, 0.0, 0.5643823935199818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Dokumen 3 (papan tulisnya putih bersih): [0.0, 0.0, 0.42328679513998635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5246530721670275, 0.5246530721670275, 0.0, 0.0, 0.5246530721670275]\n",
      "Dokumen 4 (kursinya kurang empuk): [0.0, 0.0, 0.0, 0.0, 0.6995374295560366, 0.0, 0.5643823935199818, 0.6995374295560366, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def TF_IDF(tokenized_docs, vocab):\n",
    "    N = len(tokenized_docs)  # jumlah dokumen\n",
    "\n",
    "    # df[t] = berapa dokumen yang mengandung kata t\n",
    "    df = {}\n",
    "    for v in vocab:\n",
    "        df[v] = 0\n",
    "    for tokens in tokenized_docs:\n",
    "        unique_tokens = set(tokens)\n",
    "        for t in unique_tokens:\n",
    "            if t in df:\n",
    "                df[t] += 1\n",
    "\n",
    "    # Matrix TF-IDF\n",
    "    tf_idf_matrix = []\n",
    "    for tokens in tokenized_docs:\n",
    "        row = []\n",
    "        total_kata = len(tokens)\n",
    "        for v in vocab:\n",
    "            # TF\n",
    "            tf = tokens.count(v) / float(total_kata) if total_kata > 0 else 0\n",
    "            # IDF\n",
    "            idf = math.log((N+1) / (df[v]+1)) + 1\n",
    "\n",
    "            tf_idf = tf * idf # Disini hasil TF dikalikan dengan IDF sesuai dengan rumus matematikanya\n",
    "            row.append(tf_idf)\n",
    "        tf_idf_matrix.append(row)\n",
    "    return tf_idf_matrix\n",
    "\n",
    "tfidf_matrix = TF_IDF(tokenized_texts, vocab_list)\n",
    "\n",
    "# Menampilkan hasil TF-IDF\n",
    "print(\"\\n=== TF-IDF Matrix ===\")\n",
    "for i, row in enumerate(tfidf_matrix):\n",
    "    print(f\"Dokumen {i} ({texts[i]}): {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada proses **Bag of Words (BOW)** terdapat 3 tahap, yaitu:\n",
    "\n",
    "1. Membuat Kosakata dengan mengumpulkan semua kata unik dari seluruh dokumen dan menyimpannya dalam bentuk *list* yang terurut.  \n",
    "2. Menghitung Frekuensi Kata dimana untuk setiap dokumen, akan dihitung berapa kali setiap kata dalam kosakata muncul. dan jika kata tidak muncul dalam dokumen, nilainya adalah 0.\n",
    "3. Membangun **Matriks BoW** yang dimana hasil perhitungan frekuensi setiap kata dalam dokumen disusun menjadi matriks. Baris merepresentasikan dokumen dan kolom merepresentasikan kata dari kosakata. dan juga nilai dalam matriks adalah frekuensi kata di dokumen tersebut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BoW Matrix ===\n",
      "Dokumen 0 (aku suka statistika): [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "Dokumen 1 (kelasnya bersih): [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Dokumen 2 (AC-nya kurang dingin): [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Dokumen 3 (papan tulisnya putih bersih): [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Dokumen 4 (kursinya kurang empuk): [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def BoW(tokenized_docs, vocab):\n",
    "    bow_matrix = []\n",
    "    for tokens in tokenized_docs:\n",
    "        row = []\n",
    "        for v in vocab:\n",
    "            count = tokens.count(v)\n",
    "            row.append(count)\n",
    "        bow_matrix.append(row)\n",
    "    return bow_matrix\n",
    "\n",
    "bow_matrix = BoW(tokenized_texts, vocab_list)\n",
    "\n",
    "# Menampilkan hasil BoW\n",
    "\n",
    "print(\"\\n=== BoW Matrix ===\")\n",
    "for i, row in enumerate(bow_matrix):\n",
    "    print(f\"Dokumen {i} ({texts[i]}): {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengubah label dari bentuk *string* ke *integer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"P\":0, \"N\":1, \"Ne\":2}\n",
    "y = [label_map[l] for l in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan split *train-test* untuk persiapan melakukan *modeling* dengan rasio 60:40. dengan 3 kalimat sebagai *data train* dan 2 kalimat sebagai *data test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_train, X_bow_test, y_train, y_test = train_test_split(bow_matrix, y, test_size=0.4, random_state=2024)\n",
    "X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.4, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan pelatihan menggunakan model **Decision Tree** untuk kedua data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=2024)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=2024)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=2024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_bow = DecisionTreeClassifier(random_state=2024)\n",
    "model_bow.fit(X_bow_train, y_train)\n",
    "\n",
    "model_tfidf = DecisionTreeClassifier(random_state=2024)\n",
    "model_tfidf.fit(X_tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan prediksi pada `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bow = model_bow.predict(X_bow_test)\n",
    "acc_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "y_pred_tfidf = model_tfidf.predict(X_tfidf_test)\n",
    "acc_tfidf = accuracy_score(y_test, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menunjukkan akurasi untuk model **Decision Tree** pada kedua metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi dengan BOW: 0.5\n",
      "Akurasi dengan TF-IDF: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Akurasi dengan BOW:\", acc_bow)\n",
    "print(\"Akurasi dengan TF-IDF:\", acc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Kesimpulan**\n",
    "\n",
    "<div align='justify'>\n",
    "&emsp;&emsp;&emsp;&emsp;\n",
    "Akurasi 0.5 menunjukkan bahwa model Decision Tree belum optimal karena keterbatasan data yang kecil, serta label yang ada 3 kelas berbeda dari 5 data yang menunjukkan kardinalitas yang tinggi dan tidak baik untuk sebuah model machine learning. Model hanya mampu menangkap pola dasar, namun belum cukup untuk prediksi yang akurat. Menambah data agar lebih banyak untuk memperkuat model atau menggunakan cross-validation mungkin dapat meningkatkan performa dari model.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
